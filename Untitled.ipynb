{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6be00dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForQuestionAnswering, T5ForConditionalGeneration\n",
    "import torch\n",
    "from docx import Document\n",
    "\n",
    "# модель hivaze/AAQG-QA-QG-FRED-T5-1.7B\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d14b7adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'docs/docs_json/Service_registratsii.json'\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    json_file = json.load(f)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "329b923d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0dbdc8f6ca34088a02a62616b88ecb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from functools import partial\n",
    "\n",
    "saved_checkpoint = './t5_model/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_checkpoint)\n",
    "model = T5ForConditionalGeneration.from_pretrained(saved_checkpoint)\n",
    "\n",
    "\n",
    "def generate_text(prompt, tokenizer, model, n=1, temperature=0.8, num_beams=3):\n",
    "    encoded_input = tokenizer.encode_plus(prompt, return_tensors='pt')\n",
    "    encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()}\n",
    "\n",
    "    resulted_tokens = model.generate(**encoded_input,\n",
    "                                   max_new_tokens=64,\n",
    "                                   do_sample=True,\n",
    "                                   num_beams=num_beams,\n",
    "                                   num_return_sequences=n,\n",
    "                                   temperature=temperature,\n",
    "                                   top_p=0.9,\n",
    "                                   top_k=50)\n",
    "    resulted_texts = tokenizer.batch_decode(resulted_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return resulted_texts\n",
    "\n",
    "generate_text = partial(generate_text, tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b19ff7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Нажмите на ссылку с ФИО в правом верхнем углу Системы: откроется форма «Персональные настройки».']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "QA_PROMPT = \"Сгенерируй ответ на вопрос по тексту. Текст: '{context}'. Вопрос: '{question}'.\"\n",
    "test_context = \"Пользователю ЛК предоставляется возможность изменять тему интерфейса Системы. Для того чтобы изменить текущую тему интерфейса нажмите на ссылку с ФИО в правом верхнем углу Системы: откроется форма «Персональные настройки». На форме «Персональные настройки» можно изменить часовой пояс, цвет темы, язык интерфейса.\"\n",
    "question = 'Как изменять тему интерфейса?'\n",
    "generate_text(QA_PROMPT.format(\n",
    "  context=test_context,\n",
    "  question=question\n",
    "), n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69766f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [563, 297, 23231, 28306, 1078, 309, 1247, 334, 28607, 18, 22324, 30, 6145, 25261, 8919, 30, 6145, 4670, 11184, 10744, 309, 30123, 35138, 290, 5118, 309, 15966, 16, 2302, 18525, 282, 35985, 451, 17597, 6340, 16, 19984, 2909, 49519, 9781, 31114, 282, 5118, 281, 20705, 3021, 4855, 3436, 4691, 16, 32983, 23360, 541, 26898, 8272, 19631, 49519, 2682, 18, 1598, 5608, 6325, 39935, 870, 19631, 49519, 2682, 473, 21, 280, 651, 1409, 280, 30818, 25261], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "QA_PROMPT = \"Сгенерируй ответ на вопрос по тексту. Текст: '{context}'. Вопрос: '{question}'.\"\n",
    "prompt = QA_PROMPT.format(context='', question='При создании заявления на регистрацию декларации о соответствии на продукцию, включенную в Единый перечень продукции, подлежащей декларированию соответствия в соответствии с Постановлением № 2425, обязательным полем для заполнения идут схемы декларирования. Из какого документа взяты эти схемы декларирования (1д – 7д)?')\n",
    "tokenizer.encode_plus(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dfd4f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus('Сколько яблок ты съел?', return_tensors='pt').replace('?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8dbd346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создайте входные данные для декодера\n",
    "decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]])\n",
    "\n",
    "# Пропустите текст через модель и получите скрытые состояния\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=inputs['input_ids'], decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n",
    "\n",
    "last_hidden_state = outputs.encoder_last_hidden_state\n",
    "\n",
    "# Преобразуйте скрытое состояние в эмбеддинг (например, усреднение по всем токенам)\n",
    "embedding = last_hidden_state.mean(dim=1)  # [batch_size, hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ab7d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ленивое разбиение на чанки для поиска релевантного куска\n",
    "def split_document(json_file, tokenizer, max_context_length=450, overlap=100):\n",
    "    # читаем файл, получаем список\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)['data']\n",
    "    \n",
    "    # разделяем на пересекающиеся чанки каждый блок (по токенам)\n",
    "    for i in range(len(data)):\n",
    "        title = data[i]['title']\n",
    "        content = data[i]['content']\n",
    "        if title not in content:\n",
    "            content = title + '. ' + content\n",
    "        content_tokens = tokenizer.encode(content)\n",
    "        if len(content_tokens) < max_context_length:\n",
    "            data[i]['content'] = [content]\n",
    "        else:\n",
    "            new_content = []\n",
    "            step = max_context_length - overlap\n",
    "            for j in range(0, len(content_tokens) - overlap, step):\n",
    "                chunk = content_tokens[j:j + max_context_length]\n",
    "                text = ''.join(tokenizer.batch_decode(chunk, skip_special_tokens=True))\n",
    "                new_content.append(text)\n",
    "            data[i]['content'] = new_content\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a24a7574",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs/docs_json/Naumen_Desk_splited.json\n",
      "docs/docs_json/Service_registratsii_splited.json\n"
     ]
    }
   ],
   "source": [
    "dir_path = 'docs/docs_json/' \n",
    "paths = ['Naumen_Desk', 'Service_registratsii']\n",
    "format_ = '.json'\n",
    "for path in paths:\n",
    "    res = os.path.join(dir_path, path) + format_\n",
    "    splited_doc = split_document(res, tokenizer)\n",
    "    json_data = {'data': splited_doc}\n",
    "    new_path = os.path.join(dir_path, path) + '_splited.json'\n",
    "    with open(new_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "    print(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94b02484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "path = 'docs/docs_json/Service_registratsii_splited_emb.json'\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    json_file = json.load(f)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c3e2706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7618016810571367"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = []\n",
    "for i in range(len(json_file)):\n",
    "    embeddings = json_file[i]['embeddings']\n",
    "    similarity = cosine_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "eb8e9e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "QA_PROMPT = \"Сгенерируй ответ на вопрос по тексту. Текст: '{context}'. Вопрос: '{question}'.\"\n",
    "test_context = \"Пользователю ЛК предоставляется возможность изменять тему интерфейса Системы. Для того чтобы изменить текущую тему интерфейса нажмите на ссылку с ФИО в правом верхнем углу Системы: откроется форма «Персональные настройки». На форме «Персональные настройки» можно изменить часовой пояс, цвет темы, язык интерфейса.\"\n",
    "question = 'Как изменять тему интерфейса?'\n",
    "prompt = QA_PROMPT.format(context=test_context, question=question)\n",
    "\n",
    "encoded_input = tokenizer.encode_plus(prompt, return_tensors='pt')\n",
    "encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**encoded_input,\n",
    "                             max_new_tokens=64,\n",
    "                             do_sample=True,\n",
    "                             num_beams=3,\n",
    "                             num_return_sequences=1,\n",
    "                             temperature=0.8,\n",
    "                             top_p=0.9,\n",
    "                             top_k=50, output_scores=True, return_dict_in_generate=True,\n",
    "                             early_stopping=True\n",
    "            )\n",
    "\n",
    "# Получаем токены и лог-вероятности\n",
    "generated_tokens = outputs.sequences[0]\n",
    "logits = outputs.scores\n",
    "\n",
    "# Вычисление уверенности ответа\n",
    "# Переведем логиты в вероятности токенов\n",
    "probabilities = [torch.softmax(logit, dim=-1) for logit in logits]\n",
    "\n",
    "# Извлечение вероятности предсказанных токенов\n",
    "predicted_probabilities = []\n",
    "for i, token_id in enumerate(generated_tokens[1:]):  # Пропускаем токен <bos>\n",
    "    predicted_probabilities.append(probabilities[i][0, token_id].item())\n",
    "\n",
    "# Уверенность ответа\n",
    "confidence_score = sum(predicted_probabilities) / len(predicted_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "661afac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9540417423615088"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Получаем токены и лог-вероятности\n",
    "generated_tokens = outputs.sequences[0]\n",
    "logits = outputs.scores\n",
    "\n",
    "# Вычисление уверенности ответа\n",
    "# Переведем логиты в вероятности токенов\n",
    "probabilities = [torch.softmax(logit, dim=-1) for logit in logits]\n",
    "\n",
    "# Извлечение вероятности предсказанных токенов\n",
    "predicted_probabilities = []\n",
    "for i, token_id in enumerate(generated_tokens[1:]):\n",
    "    predicted_probabilities.append(max(probabilities[i][:, token_id]).item())\n",
    "\n",
    "# Уверенность ответа\n",
    "confidence_score = sum(predicted_probabilities) / len(predicted_probabilities)\n",
    "confidence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "e9aaf91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7200158834457397"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(probabilities[0][:, 2176]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "661e8172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2176, 14465,   309, 18899,   281, 18151,   630,   282, 12233, 34258,\n",
       "         9941, 44026,     2])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8f6f838f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7200)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities[0][0, 2176]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2843a66a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5880\\2343239697.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Пропускаем токен <bos>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprobabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mpredicted_probabilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Уверенность ответа\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 0"
     ]
    }
   ],
   "source": [
    "generated_tokens = outputs.sequences[0]\n",
    "logits = outputs.scores\n",
    "\n",
    "# Вычисление уверенности ответа\n",
    "# Переведем логиты в вероятности токенов\n",
    "probabilities = [torch.softmax(logit, dim=-1) for logit in logits]\n",
    "\n",
    "# Извлечение вероятности предсказанных токенов\n",
    "predicted_probabilities = []\n",
    "for i, token_id in enumerate(generated_tokens):  # Пропускаем токен <bos>\n",
    "    probabilities = torch.max(probabilities[i], 0).values\n",
    "    predicted_probabilities.append(probabilities[i][token_id].item())\n",
    "\n",
    "# Уверенность ответа\n",
    "confidence_score = sum(predicted_probabilities) / len(predicted_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b22fb246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "34a9b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_utils import get_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_answer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
